{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import utils\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    def init_weights(shape, name = 'weights'):\n",
    "        return tf.Variable(tf.truncated_normal(shape, stddev = 0.1), name = name)\n",
    "\n",
    "    def init_biases(shape, name = 'biases'):\n",
    "        return tf.Variable(tf.constant(0.1, shape = shape), name = name)\n",
    "    \n",
    "    def __init__(self, n_visible = 784, n_hidden = 500, k = 1):\n",
    "        \n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.k = k\n",
    "        \n",
    "        print('Start building computation graph...')\n",
    "        \n",
    "        self.learning_rate = tf.placeholder(dtype = tf.float32)\n",
    "        \n",
    "        # weights\n",
    "        self.w = RBM.init_weights([n_visible, n_hidden], name = 'w')\n",
    "        # biases\n",
    "        self.vb = RBM.init_biases([n_visible], name = 'vb')\n",
    "        self.hb = RBM.init_biases([n_hidden], name = 'hb')\n",
    "\n",
    "    def forward(self, visible):\n",
    "        f_ = tf.matmul(visible, self.w) + self.hb\n",
    "        return tf.nn.sigmoid(f_)\n",
    "\n",
    "    def backward(self, hidden):\n",
    "        b_ = tf.matmul(hidden, tf.transpose(self.w)) + self.vb\n",
    "        return tf.nn.sigmoid(b_)\n",
    "    \n",
    "    def sample_h_given_v(self, visible_sample):\n",
    "        hidden_probs = self.forward(visible_sample)\n",
    "        hidden_sample = tf.nn.relu(tf.sign(hidden_probs - tf.random_uniform(tf.shape(hidden_probs))))\n",
    "        return hidden_sample\n",
    "\n",
    "    def sample_v_given_h(self, hidden_sample):\n",
    "        visible_probs = self.backward(hidden_sample)\n",
    "        visible_sample = tf.nn.relu(tf.sign(visible_probs - tf.random_uniform(tf.shape(visible_probs))))\n",
    "        return visible_sample\n",
    "\n",
    "    def CD_k(self, visible):\n",
    "        visible_sample = visible\n",
    "        hidden_sample = self.sample_h_given_v(visible_sample)\n",
    "        \n",
    "        for i in range(self.k):\n",
    "            visible_sample = self.sample_v_given_h(hidden_sample)\n",
    "            hidden_sample = self.sample_h_given_v(visible_sample)\n",
    "            \n",
    "        hidden_probs = self.forward(visible)\n",
    "        \n",
    "        # gradients for weights\n",
    "        positive_phase = tf.matmul(tf.transpose(visible), hidden_probs)\n",
    "        negative_phase = tf.matmul(tf.transpose(visible_sample), hidden_sample)\n",
    "        w_grad = (positive_phase - negative_phase) / tf.to_float(tf.shape(visible)[0])\n",
    "        \n",
    "        # gradients for biases\n",
    "        vb_grad = tf.reduce_mean(visible - visible_sample, axis = 0)\n",
    "        hb_grad = tf.reduce_mean(hidden_probs - hidden_sample, axis = 0)\n",
    "        \n",
    "        return w_grad, vb_grad, hb_grad\n",
    "    \n",
    "    def updater(self, visible):\n",
    "        w_grad, vb_grad, hb_grad = self.CD_k(visible)\n",
    "        \n",
    "        # operators\n",
    "        update_w = tf.assign(self.w, self.w + self.learning_rate * w_grad)\n",
    "        update_vb = tf.assign(self.vb, self.vb + self.learning_rate * vb_grad)        \n",
    "        update_hb = tf.assign(self.hb, self.hb + self.learning_rate * hb_grad)        \n",
    "\n",
    "        return [update_w, update_vb, update_hb]\n",
    "    \n",
    "    def free_energy(self, visible):\n",
    "        visible_term = tf.matmul(visible, tf.reshape(self.vb, [tf.shape(self.vb)[0], 1]))\n",
    "        hidden_term = tf.reduce_sum(tf.log(1 + tf.exp(tf.matmul(visible, self.w) + self.hb)), axis = 1)\n",
    "        return - visible_term - hidden_term\n",
    "    \n",
    "    def pseudo_likelihood(self, visible):\n",
    "        x = tf.round(visible)\n",
    "        free_energy_x = self.free_energy(x)\n",
    "        split0, split1, split2 = tf.split(x, [self.i, 1, tf.shape(x)[1] - self.i - 1], axis = 1)\n",
    "        xi = tf.concat([split0, 1 - split1, split2], axis = 1)\n",
    "        free_energy_xi = self.free_energy(xi)\n",
    "        self.i = (self.i + 1) % self.n_visible\n",
    "        return tf.reduce_mean(self.n_visible * tf.log(tf.nn.sigmoid(free_energy_xi - free_energy_x)), axis = 0)\n",
    "\n",
    "    def sampler(self, visible, steps = 15):\n",
    "        v_sample = visible\n",
    "        for i in range(steps):\n",
    "            v_sample = self.sample_v_given_h(self.sample_h_given_v(v_sample))\n",
    "        return v_sample\n",
    "    \n",
    "    def train(self, data, epochs, batch_size, learning_rate):\n",
    "        sample_dir = './sample'\n",
    "        \n",
    "        x = tf.placeholder(dtype = tf.float32, shape = [None, self.n_visible])\n",
    "        test_x, test_y = data.sample_batch()\n",
    "        \n",
    "        # operators\n",
    "        updates = self.updater(x)\n",
    "        pseudo_likelihood = self.pseudo_likelihood(x)\n",
    "        sampler = self.sampler(x)\n",
    "        \n",
    "        epoch_cost = []\n",
    "        epoch = 1\n",
    "        with tf.Session() as sess:\n",
    "   \n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            print('Computation graph complied...')\n",
    "            \n",
    "            for i in range(epochs * data.batch_num):\n",
    "                \n",
    "                if i % 500 == 0:\n",
    "                    samples = sess.run(sampler, feed_dict = {x: test_x})\n",
    "                    samples = samples.reshape([data.batch_size, 28, 28])\n",
    "                    utils.save_images(samples, [8, 8], os.path.join(sample_dir, 'iteration_%d.png' % i))\n",
    "                    \n",
    "                \n",
    "                batch_x, _ = data.next_batch()\n",
    "                sess.run(updates, feed_dict = {x: batch_x, self.learning_rate: learning_rate})\n",
    "                iter_cost = sess.run(pseudo_likelihood, feed_dict = {x: batch_x})\n",
    "                epoch_cost.append(iter_cost)\n",
    "                \n",
    "                if i is not 0 and data.batch_index is 0:\n",
    "                    print('Epoch %d Cost %g' % (epoch, np.mean(epoch_cost)))\n",
    "                    epoch_cost = []\n",
    "                    epoch += 1\n",
    "        print(test_y)\n",
    "                \n",
    "train_dir = {\n",
    "        'X': './mnist/train-images-idx3-ubyte.gz',\n",
    "        'Y': './mnist/train-labels-idx1-ubyte.gz'\n",
    "        }        \n",
    "            \n",
    "data = utils.DataSet(data_dir = train_dir, batch_size = 64, one_hot = False)\n",
    "\n",
    "rbm = RBM(n_visible = 784, n_hidden = 500, k = 15)\n",
    "rbm.train(data = data, epochs = 30, batch_size = 64, learning_rate = 0.01)\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    \n",
    "        \n",
    "    global train_samples, train_labels, test_samples, test_labels\n",
    "\n",
    "    train_samples = open('/home/libedev/mute/mute-hero/download/dataset/new_train_samples.txt').read().strip().split('\\n')\n",
    "    #train_labels = [int(label) for label in open('/home/libedev/mute/mute-hero/download/dataset/new_train_labels.txt').read().strip().split('\\n')]   \n",
    "    \n",
    "    train_labels = np.genfromtxt('/home/libedev/mute/mute-hero/download/dataset/new_train_labels.txt', encoding='ascii',dtype=int)\n",
    "\n",
    "    test_samples = open('/home/libedev/mute/mute-hero/download/dataset/new_test_samples.txt').read().strip().split('\\n')\n",
    "    #test_labels = [int(label) for label in open('/home/libedev/mute/mute-hero/download/dataset/new_test_labels.txt').read().strip().split('\\n')]\n",
    "    \n",
    "    test_labels=np.genfromtxt('/home/libedev/mute/mute-hero/download/dataset/new_test_labels.txt', encoding='ascii',dtype=int)\n",
    "\n",
    "\n",
    "    \n",
    "def get_random_sample(part,option):\n",
    "    \n",
    "    \n",
    "    global train_samples, train_labels, test_samples, test_labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_samples = open('/home/libedev/mute/mute-hero/download/dataset/new_train_samples.txt').read().strip().split('\\n')\n",
    "    #train_labels = [int(label) for label in open('/home/libedev/mute/mute-hero/download/dataset/new_train_labels.txt').read().strip().split('\\n')]     \n",
    "\n",
    "    train_labels = np.genfromtxt('/home/libedev/mute/mute-hero/download/dataset/new_train_labels.txt', encoding='ascii',dtype=int)\n",
    "\n",
    "    test_samples = open('/home/libedev/mute/mute-hero/download/dataset/new_test_samples.txt').read().strip().split('\\n')\n",
    "    #test_labels = [int(label) for label in open('/home/libedev/mute/mute-hero/download/dataset/new_test_labels.txt').read().strip().split('\\n')]\n",
    "\n",
    "    test_labels = np.genfromtxt('/home/libedev/mute/mute-hero/download/dataset/new_test_labels.txt', encoding='ascii',dtype=int)\n",
    "      \n",
    "    if part == 'new_train':\n",
    "        \n",
    "        samples = train_samples\n",
    "        labels = train_labels   \n",
    "        \n",
    "    elif part == 'new_test':\n",
    "        \n",
    "        samples = test_samples\n",
    "        labels = test_labels\n",
    "        \n",
    "    else :\n",
    "        \n",
    "        print('Please use train, valid, or test for the part name')\n",
    "\n",
    "    i = random.randrange(len(samples))\n",
    "    spectrum = np.load('/home/libedev/mute/mute-hero/download/dataset/'\n",
    "                           +str(part)+'/'+str(option)+'/'+samples[i]+'.npy')\n",
    "        \n",
    "        \n",
    "    return spectrum, int(labels[i])        \n",
    "\n",
    "    \n",
    "    \n",
    "def get_random_batch(part,option):\n",
    "    \n",
    "    \n",
    "    global train_samples, train_labels, test_samples, test_labels\n",
    "    \n",
    "    #option = 'spectrum_Stft'\n",
    "\n",
    "    if part == 'new_train':\n",
    "        \n",
    "        data_amount = len(train_samples)\n",
    "        example_data = np.load('/home/libedev/mute/mute-hero/download/dataset/'\n",
    "                               +str(part)+'/'+str(option)+'/'+train_samples[0]+'.npy')\n",
    "    else :\n",
    "        \n",
    "        data_amount = len(test_samples)\n",
    "        example_data = np.load('/home/libedev/mute/mute-hero/download/dataset/'\n",
    "                               +str(part)+'/'+str(option)+'/'+test_samples[0]+'.npy')\n",
    "    \n",
    "    \n",
    "    X = np.zeros((data_amount, example_data.shape[0], example_data.shape[1], 1))\n",
    "    Y = np.zeros((data_amount,))\n",
    "    \n",
    "    for i in range(data_amount):\n",
    "        \n",
    "        s,l = get_random_sample(part,option)\n",
    "        \n",
    "        X[i, :, :, 0] = s[:example_data.shape[0], :example_data.shape[1]]\n",
    "        Y[i] =  l\n",
    "        #Y = Y.astype('int16')\n",
    "        f_labels = np.save('/home/libedev/mute/mute-hero/download/dataset/'+str(option)+'.npy',Y)\n",
    "        new_Y = np.load('/home/libedev/mute/mute-hero/download/dataset/'+str(option)+'.npy')\n",
    "        \n",
    "    return X,new_Y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 3, 3, 6, 2, 7, 6, 7, 3, 6, 0, 4, 3, 7, 0, 5, 1, 4, 7, 7, 6, 3,\n",
      "        0, 4, 6, 5, 7, 7, 1, 4, 4, 2, 4, 4, 4, 7, 2, 0, 7, 6, 6, 2, 2, 0, 0, 4,\n",
      "        3, 7, 6, 5, 5, 4, 1, 3, 2, 4, 6, 6, 6, 1, 4, 1, 7, 6, 0, 5, 7, 4, 1, 7,\n",
      "        6, 6, 1, 1, 1, 5, 0, 5, 2, 1, 0, 1, 4, 4, 2, 3, 5, 3, 3, 5, 7, 1, 2, 3,\n",
      "        5, 6, 5, 1, 2, 4, 6, 0, 7, 4, 7, 7, 6, 7, 6, 6, 7, 7, 0, 5, 6, 0, 0, 3,\n",
      "        7, 0, 2, 3, 4, 7, 2, 6, 1, 2, 7, 5, 0, 5, 6, 7, 3, 2, 6, 4, 1, 7, 7, 2,\n",
      "        0, 4, 1, 4, 7, 7, 3, 4, 4, 1, 7, 2, 4, 7, 1, 1, 6, 7, 1, 1, 1, 2, 1, 1,\n",
      "        1, 3, 7, 2, 0, 4, 4, 2, 7, 0, 4, 1, 1, 0, 4, 7, 2, 4, 1, 4, 5, 2, 4, 6,\n",
      "        0, 1, 6, 6, 0, 4, 0, 7, 6, 5, 4, 1, 6, 5, 0, 4, 7, 1, 6, 0, 7, 2, 7, 5,\n",
      "        4, 4, 5, 5, 2, 4, 2, 7, 2, 7, 6, 3, 4, 0, 7, 6, 5, 6, 5, 6, 0, 7, 0, 6,\n",
      "        1, 3, 2, 7, 7, 2, 1, 0, 5, 6, 2, 4, 2, 1, 2, 0, 6, 1, 5, 2, 0, 1, 4, 1,\n",
      "        3, 6, 2, 7, 7, 2, 2, 4, 5, 5, 4, 6, 1, 6, 0, 3, 1, 0, 2, 6, 2, 5, 7, 7,\n",
      "        3, 2, 3, 5, 1, 7, 3, 4, 7, 5, 6, 0, 1, 1, 1, 2, 3, 1, 5, 3, 4, 6, 5, 3,\n",
      "        5, 1, 7, 5, 0, 3, 5, 1, 1, 4, 3, 5, 3, 3, 4, 3, 0, 4, 0, 2, 7, 6, 3, 6,\n",
      "        7, 0, 3, 2, 3, 7, 4, 5, 4, 3, 3, 2, 0, 7, 0, 3, 0, 7, 5, 2, 6, 4, 5, 2,\n",
      "        4, 3, 0, 3, 0, 6, 3, 1, 3, 4, 0, 4, 0, 3, 5, 4, 2, 3, 1, 0, 4, 4, 7, 6,\n",
      "        1, 2, 7, 0, 3, 4, 4, 3, 3, 4, 2, 4, 1, 3, 5, 7], dtype=torch.int16)\n",
      "torch.int16\n"
     ]
    }
   ],
   "source": [
    "prepare_data()\n",
    "a,b=get_random_batch(part='new_train',option='Mel_S_pca')\n",
    "b=np.array(b,np.int16)\n",
    "train_label = torch.as_tensor(b)\n",
    "print(train_label)\n",
    "print(train_label.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load('/home/libedev/mute/mute-hero/download/dataset/new_train_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 7, 4, 4, 0, 6, 7, 5, 1, 2, 7, 6, 6, 2, 5, 3, 0, 7, 7, 4, 5, 3, 3, 1,\n",
      "        0, 1, 3, 7, 3, 5, 6, 0, 3, 0, 1, 3, 0, 7, 0, 2, 2, 0, 3, 4, 4, 4, 4, 7,\n",
      "        3, 4, 2, 7, 0, 7, 0, 0, 5, 2, 3, 5, 7, 5, 1, 7, 3, 6, 4, 5, 2, 0, 4, 3,\n",
      "        1, 4, 6, 1, 7, 1, 2, 2, 2, 1, 3, 7, 7, 6, 0, 7, 0, 3, 6, 3, 1, 0, 5, 4,\n",
      "        4, 3, 5, 3, 1, 7, 5, 2, 0, 6, 0, 7, 4, 4, 1, 1, 6, 3, 0, 1, 1, 3, 5, 7,\n",
      "        7, 5, 0, 5, 7, 2, 6, 4, 3, 4, 3, 1, 7, 2, 4, 4, 7, 1, 0, 4, 5, 7, 3, 4,\n",
      "        0, 1, 7, 3, 0, 7, 6, 4, 3, 6, 2, 2, 2, 6, 2, 0, 7, 7, 6, 6, 1, 5, 4, 5,\n",
      "        5, 4, 3, 4, 4, 4, 6, 7, 5, 3, 3, 6, 5, 6, 2, 1, 7, 3, 2, 0, 0, 1, 7, 0,\n",
      "        0, 0, 6, 6, 1, 6, 1, 2, 1, 6, 5, 7, 2, 5, 3, 3, 3, 1, 4, 0, 2, 3, 7, 7,\n",
      "        5, 3, 1, 7, 1, 0, 1, 7, 6, 0, 4, 2, 2, 0, 5, 0, 7, 1, 4, 6, 7, 3, 5, 6,\n",
      "        0, 0, 4, 5, 4, 3, 2, 6, 0, 2, 5, 3, 4, 4, 2, 2, 3, 5, 2, 6, 1, 4, 1, 6,\n",
      "        3, 7, 2, 5, 2, 6, 7, 5, 6, 4, 2, 5, 6, 2, 2, 3, 5, 2, 4, 2, 5, 6, 4, 0,\n",
      "        5, 1, 2, 3, 5, 4, 5, 0, 1, 7, 1, 4, 4, 4, 3, 3, 4, 6, 6, 0, 3, 5, 6, 1,\n",
      "        2, 4, 4, 6, 0, 6, 2, 3, 3, 4, 7, 2, 3, 3, 6, 1, 5, 4, 2, 2, 5, 0, 0, 6,\n",
      "        5, 1, 1, 6, 1, 6, 4, 4, 2, 0, 5, 2, 1, 3, 0, 1, 1, 1, 1, 5, 2, 2, 0, 0,\n",
      "        5, 5, 0, 1, 7, 7, 1, 0, 7, 3, 7, 7, 6, 5, 2, 0, 6, 0, 6, 1, 1, 6, 4, 2,\n",
      "        5, 1, 6, 7, 5, 5, 5, 6, 6, 2, 6, 2, 7, 5, 1, 3])\n",
      "tensor([7, 7, 4, 4, 0, 6, 7, 5, 1, 2, 7, 6, 6, 2, 5, 3, 0, 7, 7, 4, 5, 3, 3, 1,\n",
      "        0, 1, 3, 7, 3, 5, 6, 0, 3, 0, 1, 3, 0, 7, 0, 2, 2, 0, 3, 4, 4, 4, 4, 7,\n",
      "        3, 4, 2, 7, 0, 7, 0, 0, 5, 2, 3, 5, 7, 5, 1, 7, 3, 6, 4, 5, 2, 0, 4, 3,\n",
      "        1, 4, 6, 1, 7, 1, 2, 2, 2, 1, 3, 7, 7, 6, 0, 7, 0, 3, 6, 3, 1, 0, 5, 4,\n",
      "        4, 3, 5, 3, 1, 7, 5, 2, 0, 6, 0, 7, 4, 4, 1, 1, 6, 3, 0, 1, 1, 3, 5, 7,\n",
      "        7, 5, 0, 5, 7, 2, 6, 4, 3, 4, 3, 1, 7, 2, 4, 4, 7, 1, 0, 4, 5, 7, 3, 4,\n",
      "        0, 1, 7, 3, 0, 7, 6, 4, 3, 6, 2, 2, 2, 6, 2, 0, 7, 7, 6, 6, 1, 5, 4, 5,\n",
      "        5, 4, 3, 4, 4, 4, 6, 7, 5, 3, 3, 6, 5, 6, 2, 1, 7, 3, 2, 0, 0, 1, 7, 0,\n",
      "        0, 0, 6, 6, 1, 6, 1, 2, 1, 6, 5, 7, 2, 5, 3, 3, 3, 1, 4, 0, 2, 3, 7, 7,\n",
      "        5, 3, 1, 7, 1, 0, 1, 7, 6, 0, 4, 2, 2, 0, 5, 0, 7, 1, 4, 6, 7, 3, 5, 6,\n",
      "        0, 0, 4, 5, 4, 3, 2, 6, 0, 2, 5, 3, 4, 4, 2, 2, 3, 5, 2, 6, 1, 4, 1, 6,\n",
      "        3, 7, 2, 5, 2, 6, 7, 5, 6, 4, 2, 5, 6, 2, 2, 3, 5, 2, 4, 2, 5, 6, 4, 0,\n",
      "        5, 1, 2, 3, 5, 4, 5, 0, 1, 7, 1, 4, 4, 4, 3, 3, 4, 6, 6, 0, 3, 5, 6, 1,\n",
      "        2, 4, 4, 6, 0, 6, 2, 3, 3, 4, 7, 2, 3, 3, 6, 1, 5, 4, 2, 2, 5, 0, 0, 6,\n",
      "        5, 1, 1, 6, 1, 6, 4, 4, 2, 0, 5, 2, 1, 3, 0, 1, 1, 1, 1, 5, 2, 2, 0, 0,\n",
      "        5, 5, 0, 1, 7, 7, 1, 0, 7, 3, 7, 7, 6, 5, 2, 0, 6, 0, 6, 1, 1, 6, 4, 2,\n",
      "        5, 1, 6, 7, 5, 5, 5, 6, 6, 2, 6, 2, 7, 5, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data)\n",
    "data=torch.as_tensor(np.array(data))\n",
    "print(data)\n",
    "data[0].dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MUTE",
   "language": "python",
   "name": "mute"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
